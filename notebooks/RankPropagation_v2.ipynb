{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from cvxopt import matrix, solvers\n",
    "import cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output in Weka format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def predict_for_test(test, predict, probability, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"=== Predictions on test data ===\\n\")\n",
    "        f.write(\" inst#     actual  predicted error prediction\\n\")\n",
    "        for i in range(len(test)):\n",
    "            string = [str(i + 1)]\n",
    "            if test[i] == 1:\n",
    "                string.append(\"1:positive\")\n",
    "            else:\n",
    "                string.append(\"2:negative\")\n",
    "            if predict[i] == 1:\n",
    "                string.append(\"1:positive\")\n",
    "            else:\n",
    "                string.append(\"2:negative\")\n",
    "            if test[i] == predict[i]:\n",
    "                string.append(\" \" * 5)\n",
    "            else:\n",
    "                string.append(\" \" * 2 + \"+\" + \" \" * 2)\n",
    "            if predict[i] == 1:\n",
    "                string.append(str(probability[i][1]))\n",
    "            else:\n",
    "                string.append(str(probability[i][0]))\n",
    "            string = \" \".join(string) + \"\\n\"\n",
    "            f.write(string)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "#import data\n",
    "def importData(path):\n",
    "    dataset = arff.load(open(path, 'rb'))\n",
    "    data = np.array(dataset['data'])\n",
    "    #print data[:10]\n",
    "\n",
    "    #extract features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        f = []\n",
    "        for i in range(len(d) - 1):\n",
    "            num = float(d[i])\n",
    "            if int(num) == num:\n",
    "                num = int(num)\n",
    "            f.append(num)\n",
    "        features.append(f)\n",
    "\n",
    "        if d[-1] == \"positive\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return np.asarray(features), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import training data and test data\n",
    "train_datapath = \"../myclassify/qa.train.arff\"\n",
    "test_datapath = \"../myclassify/qa.test.arff\"\n",
    "\n",
    "X_train, y_train = importData(train_datapath)\n",
    "X_test, y_test = importData(test_datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.99      0.91      1233\n",
      "          1       0.82      0.17      0.28       284\n",
      "\n",
      "avg / total       0.83      0.84      0.79      1517\n",
      "\n",
      "[[1222   11]\n",
      " [ 235   49]]\n",
      "Accuracy: 0.837837837838\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "# Jacana features only\n",
    "clf = linear_model.LogisticRegression(C=0.01,\n",
    "#                                          penalty=\"l1\"\n",
    "                                        )\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map each word to an index\n",
    "ndim = 300\n",
    "glove_path = \"../data/glove_embeddings/glove.6B.{}d.txt\".format(ndim)\n",
    "with open(glove_path, \"rb\") as lines:\n",
    "    w2idx = {line.split()[0].decode(\"utf-8\"): i for i, line in enumerate(lines)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = np.empty((len(w2idx), ndim), dtype=np.float)\n",
    "with open(glove_path, \"rb\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "        vectors[i] = np.asarray(map(float, line.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words_to_exclude = frozenset(string.punctuation) | frozenset([\"..\", \"...\"])\n",
    "words_to_exclude |= frozenset(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rank propagation\n",
    "\n",
    "---\n",
    "Get vector indicating which example belongs to which question group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_QA_group_count(infile):        \n",
    "    line = infile.readline().strip()\n",
    "    if line == \"\":\n",
    "        return None\n",
    "    \n",
    "    if not line.startswith(\"<QApairs\"):\n",
    "        raise Exception(\"Invalid data format: {}<-----\".format(line))\n",
    "    \n",
    "    sentence_count = 0\n",
    "    while not line.strip().startswith(\"</QApairs\"):\n",
    "        line = infile.readline().replace('\\t', ' ')                    \n",
    "        if line.strip().lower().startswith(\"<positive\") or line.strip().lower().startswith(\"<negative\"):\n",
    "            sentence_count += 1\n",
    "    \n",
    "    return sentence_count\n",
    "\n",
    "def get_QA_group_indicators(filepath):    \n",
    "    with open(filepath) as infile:\n",
    "        indicators = []\n",
    "        qn_number = 0\n",
    "        while infile:\n",
    "            count = get_QA_group_count(infile)\n",
    "\n",
    "            # Check for EOF\n",
    "            if count == None:\n",
    "                break\n",
    "\n",
    "            if count > 0:\n",
    "                indicators += ([qn_number] * count)\n",
    "                qn_number += 1\n",
    "        \n",
    "    return np.asarray(indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a question and its candidate answers\n",
    "def get_QA_group(infile):\n",
    "    question = []\n",
    "    answers = []\n",
    "    line = infile.readline().strip()\n",
    "    if line == \"\":\n",
    "        return None\n",
    "    \n",
    "    if not line.startswith(\"<QApairs\"):\n",
    "        raise Exception(\"Invalid data format: {}<-----\".format(line))\n",
    "        \n",
    "    while not line.strip().startswith(\"</QApairs\"):\n",
    "        line = infile.readline().replace('\\t', ' ')\n",
    "        if line.strip().lower().startswith(\"<question\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            question.append(line.strip())\n",
    "        elif line.strip().lower().startswith(\"<positive\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            answers.append((\"positive\", line.strip()))\n",
    "        elif line.strip().lower().startswith(\"<negative\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            answers.append((\"negative\", line.strip()))\n",
    "    \n",
    "    return {\"question\": question, \"answers\": answers}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def extract_vector(sentence, exclude, w2idx, wordvectors):\n",
    "    \"\"\"Compute the vector for a sentence by averaging the words in the sentence that has word embeddings\"\"\"\n",
    "    # Tokenize sentence\n",
    "    splitter = WhitespaceTokenizer()\n",
    "    tokens = splitter.tokenize(sentence)    \n",
    "    # Remove stopwords and punctuation\n",
    "    words = [t.lower() for t in tokens if t.lower() not in exclude ]\n",
    "    \n",
    "    # If we cannot find any words, we can consider returning a vector of 0\n",
    "    # and set the resulting cosine similarity to 0 otherwise will result in nan\n",
    "    # because cosine similarity will divide by 0.\n",
    "    assert(len(words) > 0)\n",
    "            \n",
    "    # Average words in sentence that are in word matrix\n",
    "    try:\n",
    "        avg_vec = np.mean([wordvectors[w2idx[w]] for w in words if w in w2idx ] \n",
    "                                                 or [np.zeros(wordvectors.shape[1])], \n",
    "                           axis=0)\n",
    "        if not np.any(avg_vec):\n",
    "            print(\"Tokens cannot be found: {}\".format(words))\n",
    "        assert(np.any(avg_vec))\n",
    "        return avg_vec\n",
    "    except UnicodeDecodeError:\n",
    "        print(line.strip())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pairwise_distance_matrix(X, k):\n",
    "    \"\"\"Compute pairwise distances between each point in X\n",
    "    and its k-nearest neighbors.\"\"\"\n",
    "\n",
    "    from scipy.spatial import KDTree\n",
    "    kdtree = KDTree(X)\n",
    "    A = np.zeros((X.shape[0], X.shape[0]), dtype=np.float)\n",
    "    for i, x in enumerate(X):\n",
    "        distances, idxs = kdtree.query(x, k+1)  # k+1 as one pt is the pt itself.\n",
    "        for d, j in zip(distances, idxs):\n",
    "            A[i, j] = d**2  # Store squared euclidean distance\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute weight matrix (i.e., the Graph Laplacian) $L$ for each set of question and its candidate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# Compute the scores/features for a dataset\n",
    "def get_weight_matrix(input_file, n_neighbors=5, sigma=1.0, eps=0.0001):\n",
    "    \"\"\"Compute weight matrix for question and answer sentences \n",
    "    \"\"\"\n",
    "    with open(input_file) as infile:\n",
    "        num_questions = 0\n",
    "        while infile:\n",
    "            group = get_QA_group(infile)\n",
    "\n",
    "            # Check for EOF\n",
    "            if group is None:\n",
    "                break\n",
    "                \n",
    "            # Extract question vector\n",
    "            question = group[\"question\"]\n",
    "            qvec = extract_vector(question[0], words_to_exclude, w2idx, vectors)\n",
    "\n",
    "            scores = []\n",
    "            answer_vectors = []\n",
    "            for (label, sentence) in group[\"answers\"]:\n",
    "                # Compute similarity with question vector\n",
    "                vec = extract_vector(sentence, words_to_exclude, w2idx, vectors) # TODO: Pass these in as args\n",
    "                answer_vectors.append(vec)\n",
    "                cosine_distance = sp.spatial.distance.cosine(qvec, vec)\n",
    "                scores.append((label, cosine_distance))\n",
    "\n",
    "            # Compute pairwise distances between the answer vectors for K nearest neighbor\n",
    "            k = min(n_neighbors, len(answer_vectors) - 1) # Minus 1 because have to exclude itself\n",
    "            # Not enough to do rank propagation. Just keep original scores.\n",
    "            if k < 0:\n",
    "                yield None, None\n",
    "            elif k == 0:\n",
    "                yield 1, None\n",
    "            else:                \n",
    "                answer_vectors = np.vstack(answer_vectors)\n",
    "                W = compute_pairwise_distance_matrix(answer_vectors, k)\n",
    "                W = np.maximum(W, W.T)  # Ensure W symmetric.\n",
    "                W[W > 0] = np.exp(- W[W > 0] / (2 * sigma**2))  # Apply gaussian kernel\n",
    "                D = np.diag(np.sum(W, axis=1))  # Row sum of W\n",
    "                L = D - W\n",
    "#                 L = L + eps * np.eye(len(answer_vectors))  # Improve the condition of the graph laplacian                \n",
    "                Dinvsqrt = np.sqrt(np.linalg.pinv(D))                \n",
    "                # Need to ensure that Dinvsqrt does not have NAN due to division by 0\n",
    "                assert(not np.any(np.isnan(Dinvsqrt)))                \n",
    "                L = Dinvsqrt.dot(L).dot(Dinvsqrt)  # Normalized graph laplacian\n",
    "                \n",
    "#                 assert(is_pos_def(Dinvsqrt))\n",
    "#                 assert(is_pos_def(L))\n",
    "                \n",
    "                yield L.shape[0], L\n",
    "            \n",
    "            num_questions += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load question answer similarity values from file (probably should compute it here).\n",
    "\n",
    "**NOTE**: Similarity is the wrong term to use here. The values in the file are actually cosine **distances**.\n",
    "To convert it to similarity, we need to subtract it from 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_similarity_features(filepath):\n",
    "    features = []\n",
    "    labels = []\n",
    "    map_label = {\"positive\": 1, \"negative\": 0}\n",
    "    with open(filepath) as infile:\n",
    "        for line in infile:\n",
    "            label, score = line.strip().split(',')\n",
    "            score = float(score)\n",
    "            label = map_label[label]\n",
    "            features.append(score)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return np.asarray(features).reshape(-1, 1), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import training data and test data\n",
    "train_datapath = \"../myclassify/qa.train.arff\"\n",
    "test_datapath = \"../myclassify/qa.test.arff\"\n",
    "\n",
    "X_train, y_train = importData(train_datapath)\n",
    "X_test, y_test = importData(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = \"../data/answerSelectionExperiments/data/train-less-than-40.xml\"\n",
    "test_file = \"../data/answerSelectionExperiments/data/test-less-than-40.xml\"\n",
    "qn_group_indicators = get_QA_group_indicators(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sim_train, y_sim_train = load_similarity_features(\"../data/features/glove_embedding_sentence_similarities_train_100.txt\")\n",
    "X_sim_test, y_sim_test = load_similarity_features(\"../data/features/glove_embedding_sentence_similarities_test_100.txt\")\n",
    "\n",
    "X_combined_train = np.hstack((X_train, X_sim_train))\n",
    "X_combined_test = np.hstack((X_test, X_sim_test))\n",
    "\n",
    "# Scale combined data\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_combined_train)\n",
    "X_comb_scaled_train = scaler.transform(X_combined_train)\n",
    "X_comb_scaled_test = scaler.transform(X_combined_test)\n",
    "\n",
    "# Only normalize the similarity scores\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_sim_train)\n",
    "X_sim_train = scaler.transform(X_sim_train)\n",
    "X_sim_test = scaler.transform(X_sim_test)\n",
    "X_comb_scaledsim_train = np.hstack((X_train, X_sim_train))\n",
    "X_comb_scaledsim_test = np.hstack((X_test, X_sim_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Propagate rank score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cvxpy import Variable, Minimize, norm, quad_form, Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    \"\"\"Check if a matrix is positive definite. For debugging purposes.\"\"\"\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Version 2 that uses the ans_type_match and question answer similarity weights\n",
    "def propagate_scores(r, L, ans_type_match, ans_sim_weights, alpha=1.0, gamma=1.0):\n",
    "    \"\"\"Solve convex optimization problem to get new scores\"\"\"\n",
    "        \n",
    "    n = r.size\n",
    "    y = Variable(n)    \n",
    "    assert(len(ans_type_match) == n and len(ans_sim_weights) == n)\n",
    "    \n",
    "    # If no type match we just ignore the type term\n",
    "    if not np.any(ans_type_match):\n",
    "        objective = Minimize( norm(r - y, 1) + alpha * quad_form(y, L) )\n",
    "    else:\n",
    "        type_term = sum( ans_sim_weights[i] * cvxpy.abs(1 - y[i]) \n",
    "                        for i, match in enumerate(ans_type_match) if match == 1 )\n",
    "        objective = Minimize( norm(r - y, 1) + alpha * quad_form(y, L) + gamma * type_term)\n",
    "            \n",
    "    constraints = [0 <= y, y <= 1]\n",
    "    prob = Problem(objective, constraints)    \n",
    "\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve(verbose=False)      \n",
    "    assert(prob.status == \"optimal\")\n",
    "    \n",
    "    return y.value.flatten().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qn_answer_match_indicators(filepath):\n",
    "    with open(filepath) as infile:\n",
    "        return np.asarray([int(x.strip()) for x in infile])\n",
    "    \n",
    "def get_qn_answer_sim_weights(filepath):\n",
    "    with open(filepath) as infile:\n",
    "        return np.asarray([float(x.split(',')[1]) for x in infile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_propagation(data_filepath, qn_match_filepath, qn_simweights_filepath, r, \n",
    "                     alpha=1.0, sigma=1.0, n_neighbors=5, gamma=1.0):\n",
    "    total_count = 0\n",
    "\n",
    "    # Get qn group indicator\n",
    "    qn_group_indicators = get_QA_group_indicators(data_filepath)\n",
    "    \n",
    "    # Get qn answer type match\n",
    "    qn_ans_type_match = get_qn_answer_match_indicators(qn_match_filepath)\n",
    "    \n",
    "    # Get qn answer similarity weights\n",
    "    # The weights are actually distances so we subtract them from 1 to convert distance to similarity\n",
    "    qn_ans_sim_weights = 1 - get_qn_answer_sim_weights(qn_simweights_filepath)\n",
    "    \n",
    "    qn_number = 0  # Current question number (NOTE: This is not ID in XML)\n",
    "\n",
    "    scores = []  # To store the final refined scores\n",
    "    # L is actually the graph Laplacian matrix\n",
    "    for (count, L) in get_weight_matrix(data_filepath, n_neighbors, sigma):\n",
    "        # Skip question without candidate answers\n",
    "        if count is None:\n",
    "            continue\n",
    "\n",
    "        # Not enough points to propagate. Just use original value.\n",
    "        MIN_NUM_CANDIDATES = 3\n",
    "        if count <= MIN_NUM_CANDIDATES:\n",
    "            assert(r[qn_group_indicators == qn_number].size == count)\n",
    "            scores += r[qn_group_indicators == qn_number].tolist()\n",
    "        else:\n",
    "            # Get indicator vector for which answer has matching type\n",
    "            ans_type_match = qn_ans_type_match[qn_group_indicators == qn_number]\n",
    "            \n",
    "            # Get question / answer similarity weights\n",
    "            ans_sim_weights = qn_ans_sim_weights[qn_group_indicators == qn_number]                        \n",
    "            \n",
    "            # Propagate and append new scores\n",
    "            assert(r[qn_group_indicators == qn_number].size == L.shape[0])\n",
    "            new_scores = propagate_scores(r[qn_group_indicators == qn_number],\n",
    "                                          L, \n",
    "                                          ans_type_match, ans_sim_weights,\n",
    "                                          alpha, gamma)                \n",
    "            scores += new_scores\n",
    "\n",
    "        qn_number += 1\n",
    "        total_count += count\n",
    "\n",
    "    return np.asarray(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=12,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=300, presort='auto', random_state=47156,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(n_estimators=300, max_depth=12, random_state=47156)\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "            criterion='entropy', max_depth=12, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
       "            oob_score=False, random_state=73514, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=400, \n",
    "                                criterion=\"entropy\", \n",
    "                                max_depth=12,\n",
    "                                class_weight=\"balanced_subsample\",\n",
    "                                random_state=73514)\n",
    "clf_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "            criterion='entropy', max_depth=12, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
       "            oob_score=False, random_state=3471, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_sim = RandomForestClassifier(n_estimators=400,\n",
    "                             max_depth=12,\n",
    "                             min_samples_split=4,\n",
    "                             criterion=\"entropy\",\n",
    "                             random_state=3471,\n",
    "                             class_weight=\"balanced_subsample\"\n",
    "                            )\n",
    "clf_rf_sim.fit(X_combined_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf_lr = linear_model.LogisticRegression(C=100000, max_iter=1000)\n",
    "\n",
    "clf_lr = linear_model.LogisticRegression(C=0.5, max_iter=1000)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf_lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838497033619\n",
      "0.828609096902\n",
      "0.0180480847953\n",
      "0.881989279342\n",
      "245\n",
      "0.161502966381\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "\n",
    "# This file contains entries indicating whether answer has a entity matching type required by answer prediction\n",
    "# Line corresponding to entry has 1 if there is a match, else 0\n",
    "qn_match_filepath = \"../data/QuestionType/test_answer_type_match.txt\"\n",
    "\n",
    "# This file contains the cosine *distance* between question and answer\n",
    "qn_simweights_filepath = \"../data/features/glove_embedding_sentence_similarities_test_300.txt\"\n",
    "\n",
    "raw_scores = clf_rf.predict_proba(X_test)[:, 1]\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "\n",
    "# I think we have to keep the number of neighbors in the graph small because there are only a few\n",
    "# positive examples. We don't want to link them to too many negative ones.\n",
    "# This works well with 300 dim glove vector\n",
    "# scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "#                           raw_scores, alpha=1, sigma=1.0, n_neighbors=3, gamma=1.5)\n",
    "\n",
    "# L2 loss for | r - y | gives 0.6377 and 0.755 \n",
    "scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "                          raw_scores, alpha=1, sigma=1.0, n_neighbors=3, gamma=1.5)\n",
    "\n",
    "y_pred_adjusted = (scores >= 0.5)\n",
    "\n",
    "print(accuracy_score(y_test, scores >= 0.5))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(np.sum(np.abs(raw_scores - scores)) / len(scores))  # Average difference between actual\n",
    "print(np.max(np.abs(raw_scores - scores)))\n",
    "print(np.sum(y_test != (scores >= 0.5)))\n",
    "print(np.sum(y_test != (scores >= 0.5)) / float(len(scores)))\n",
    "\n",
    "print(np.sum(np.abs(y_pred - y_pred_adjusted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converts to weka format\n",
    "P = np.hstack(((1 - scores).reshape(-1, 1), scores.reshape(-1, 1)))\n",
    "predict_for_test(y_test, y_pred_adjusted, P, \"rf_type_term_adjusted.txt\")\n",
    "predict_for_test(y_test, y_pred, clf_lr.predict_proba(X_test), \"rf_no_type_term.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached EOF at qn 93\n",
      "one_count: 5 total_count: 4718\n",
      "0.93069097075\n",
      "0.00652913182896\n"
     ]
    }
   ],
   "source": [
    "# For training\n",
    "train_raw_scores = clf.predict_proba(X_train)[:, 1]\n",
    "scores = rank_propagation(train_file, train_raw_scores, alpha=2, sigma=2, n_neighbors=11)\n",
    "\n",
    "print(accuracy_score(y_train, scores >= 0.5))\n",
    "print(np.sum(np.abs(train_raw_scores - scores)) / len(scores))  # Average difference between actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93535396354387457"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_adjusted = (scores >= 0.5)\n",
    "np.sum(y_pred - y_pred_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16545814106789716"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "251.0 / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83783783783783783"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvxpy.abs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
