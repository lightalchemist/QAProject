{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from cvxopt import matrix, solvers\n",
    "import cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output in Weka format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def predict_for_test(test, predict, probability, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"=== Predictions on test data ===\\n\")\n",
    "        f.write(\" inst#     actual  predicted error prediction\\n\")\n",
    "        for i in range(len(test)):\n",
    "            string = [str(i + 1)]\n",
    "            if test[i] == 1:\n",
    "                string.append(\"1:positive\")\n",
    "            else:\n",
    "                string.append(\"2:negative\")\n",
    "            if predict[i] == 1:\n",
    "                string.append(\"1:positive\")\n",
    "            else:\n",
    "                string.append(\"2:negative\")\n",
    "            if test[i] == predict[i]:\n",
    "                string.append(\" \" * 5)\n",
    "            else:\n",
    "                string.append(\" \" * 2 + \"+\" + \" \" * 2)\n",
    "            if predict[i] == 1:\n",
    "                string.append(str(probability[i][1]))\n",
    "            else:\n",
    "                string.append(str(probability[i][0]))\n",
    "            string = \" \".join(string) + \"\\n\"\n",
    "            f.write(string)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "#import data\n",
    "def importData(path):\n",
    "    dataset = arff.load(open(path, 'rb'))\n",
    "    data = np.array(dataset['data'])\n",
    "    #print data[:10]\n",
    "\n",
    "    #extract features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        f = []\n",
    "        for i in range(len(d) - 1):\n",
    "            num = float(d[i])\n",
    "            if int(num) == num:\n",
    "                num = int(num)\n",
    "            f.append(num)\n",
    "        features.append(f)\n",
    "\n",
    "        if d[-1] == \"positive\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return np.asarray(features), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import training data and test data\n",
    "train_datapath = \"../myclassify/qa.train.arff\"\n",
    "test_datapath = \"../myclassify/qa.test.arff\"\n",
    "\n",
    "X_train, y_train = importData(train_datapath)\n",
    "X_test, y_test = importData(test_datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.99      0.91      1233\n",
      "          1       0.82      0.17      0.28       284\n",
      "\n",
      "avg / total       0.83      0.84      0.79      1517\n",
      "\n",
      "[[1222   11]\n",
      " [ 235   49]]\n",
      "Accuracy: 0.837837837838\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "# Jacana features only\n",
    "clf = linear_model.LogisticRegression(C=0.01,\n",
    "#                                          penalty=\"l1\"\n",
    "                                        )\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map each word to an index\n",
    "ndim = 300\n",
    "glove_path = \"../data/glove_embeddings/glove.6B.{}d.txt\".format(ndim)\n",
    "with open(glove_path, \"rb\") as lines:\n",
    "    w2idx = {line.split()[0].decode(\"utf-8\"): i for i, line in enumerate(lines)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = np.empty((len(w2idx), ndim), dtype=np.float)\n",
    "with open(glove_path, \"rb\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "        vectors[i] = np.asarray(map(float, line.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words_to_exclude = frozenset(string.punctuation) | frozenset([\"..\", \"...\"])\n",
    "words_to_exclude |= frozenset(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Get vector indicating which example belongs to which question group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_QA_group_count(infile):        \n",
    "    line = infile.readline().strip()\n",
    "    if line == \"\":\n",
    "        return None\n",
    "    \n",
    "    if not line.startswith(\"<QApairs\"):\n",
    "        raise Exception(\"Invalid data format: {}<-----\".format(line))\n",
    "    \n",
    "#     print(line)\n",
    "    sentence_count = 0\n",
    "    while not line.strip().startswith(\"</QApairs\"):\n",
    "        line = infile.readline().replace('\\t', ' ')                    \n",
    "        if line.strip().lower().startswith(\"<positive\") or line.strip().lower().startswith(\"<negative\"):\n",
    "            sentence_count += 1\n",
    "    \n",
    "    return sentence_count\n",
    "\n",
    "def get_QA_group_indicators(filepath):    \n",
    "    with open(filepath) as infile:\n",
    "        indicators = []\n",
    "        qn_number = 0\n",
    "        while infile:\n",
    "            count = get_QA_group_count(infile)\n",
    "\n",
    "            # Check for EOF\n",
    "            if count == None:\n",
    "#                 print(\"Reached EOF at qn {}\".format(qn_number))\n",
    "                break\n",
    "\n",
    "            if count > 0:\n",
    "                indicators += ([qn_number] * count)\n",
    "                qn_number += 1\n",
    "        \n",
    "    return np.asarray(indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Compute weight matrix $W$ for each set of question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a question and its candidate answers\n",
    "def get_QA_group(infile):\n",
    "    question = []\n",
    "    answers = []\n",
    "    line = infile.readline().strip()\n",
    "    if line == \"\":\n",
    "        return None\n",
    "    \n",
    "    if not line.startswith(\"<QApairs\"):\n",
    "        raise Exception(\"Invalid data format: {}<-----\".format(line))\n",
    "    \n",
    "#     print(line.strip())\n",
    "    \n",
    "    while not line.strip().startswith(\"</QApairs\"):\n",
    "        line = infile.readline().replace('\\t', ' ')\n",
    "        if line.strip().lower().startswith(\"<question\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            question.append(line.strip())\n",
    "        elif line.strip().lower().startswith(\"<positive\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            answers.append((\"positive\", line.strip()))\n",
    "        elif line.strip().lower().startswith(\"<negative\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            answers.append((\"negative\", line.strip()))\n",
    "    \n",
    "    return {\"question\": question, \"answers\": answers}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def extract_vector(sentence, exclude, w2idx, wordvectors):\n",
    "    \"\"\"Compute the vector for a sentence by averaging the words in the sentence that has word embeddings\"\"\"\n",
    "    # Tokenize sentence\n",
    "    splitter = WhitespaceTokenizer()\n",
    "    tokens = splitter.tokenize(sentence)    \n",
    "    # Remove stopwords and punctuation\n",
    "    words = [t.lower() for t in tokens if t.lower() not in exclude ]\n",
    "    \n",
    "    # If we cannot find any words, we can consider returning a vector of 0\n",
    "    # and set the resulting cosine similarity to 0 otherwise will result in nan\n",
    "    # because cosine similarity will divide by 0.\n",
    "    assert(len(words) > 0)\n",
    "            \n",
    "    # Average words in sentence that are in word matrix\n",
    "    try:\n",
    "        avg_vec = np.mean([wordvectors[w2idx[w]] for w in words if w in w2idx ] \n",
    "                                                 or [np.zeros(wordvectors.shape[1])], \n",
    "                           axis=0)\n",
    "        if not np.any(avg_vec):\n",
    "            print(\"Tokens cannot be found: {}\".format(words))\n",
    "        assert(np.any(avg_vec))\n",
    "        return avg_vec\n",
    "    except UnicodeDecodeError:\n",
    "        print(line.strip())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pairwise_distance_matrix(X, k):\n",
    "    \"\"\"Compute pairwise distances between each point in X\n",
    "    and its k-nearest neighbors.\"\"\"\n",
    "\n",
    "    from scipy.spatial import KDTree\n",
    "    kdtree = KDTree(X)\n",
    "    A = np.zeros((X.shape[0], X.shape[0]), dtype=np.float)\n",
    "    for i, x in enumerate(X):\n",
    "        distances, idxs = kdtree.query(x, k+1)  # k+1 as one pt is the pt itself.\n",
    "        for d, j in zip(distances, idxs):\n",
    "            A[i, j] = d**2  # Store squared euclidean distance\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# Compute the scores/features for a dataset\n",
    "def get_weight_matrix(input_file, n_neighbors=5, sigma=1.0, eps=0.0001):\n",
    "    \"\"\"Compute weight matrix for question and answer sentences \n",
    "    \"\"\"\n",
    "    with open(input_file) as infile:\n",
    "        num_questions = 0\n",
    "        while infile:\n",
    "            group = get_QA_group(infile)\n",
    "\n",
    "            # Check for EOF\n",
    "            if group is None:\n",
    "                break\n",
    "                \n",
    "            # Extract question vector\n",
    "            question = group[\"question\"]\n",
    "            qvec = extract_vector(question[0], words_to_exclude, w2idx, vectors)\n",
    "\n",
    "            scores = []\n",
    "            answer_vectors = []\n",
    "            for (label, sentence) in group[\"answers\"]:\n",
    "                # Compute similarity with question vector\n",
    "                vec = extract_vector(sentence, words_to_exclude, w2idx, vectors) # TODO: Pass these in as args\n",
    "                answer_vectors.append(vec)\n",
    "                cosine_distance = sp.spatial.distance.cosine(qvec, vec)\n",
    "                scores.append((label, cosine_distance))\n",
    "\n",
    "            # Compute pairwise distances between the answer vectors for K nearest neighbor\n",
    "            k = min(n_neighbors, len(answer_vectors) - 1) # Minus 1 because have to exclude itself\n",
    "            # Not enough to do rank propagation. Just keep original scores.\n",
    "            if k < 0:\n",
    "#                 print(\"---> No candidate answers\")\n",
    "                yield None, None\n",
    "            elif k == 0:\n",
    "#                 print(\"---> Not enough candidate answers to do rank propagation. k = {} <---\".format(k))\n",
    "                yield 1, None\n",
    "            else:                \n",
    "#                 print(\"k = {}\".format(k))            \n",
    "                answer_vectors = np.vstack(answer_vectors)\n",
    "#                 print(answer_vectors.shape)\n",
    "                W = compute_pairwise_distance_matrix(answer_vectors, k)\n",
    "                W = np.maximum(W, W.T)  # Ensure W symmetric.\n",
    "                W[W > 0] = np.exp(- W[W > 0] / (2 * sigma**2))  # Apply gaussian kernel\n",
    "                D = np.diag(np.sum(W, axis=1))  # Row sum of W\n",
    "                L = D - W\n",
    "\n",
    "#                 L = L + eps * np.eye(len(answer_vectors))  # Improve the condition of the graph laplacian\n",
    "                \n",
    "                Dinvsqrt = np.sqrt(np.linalg.pinv(D))\n",
    "                \n",
    "                # Need to ensure that Dinvsqrt does not have NAN due to division by 0\n",
    "                assert(not np.any(np.isnan(Dinvsqrt)))\n",
    "                \n",
    "                L = Dinvsqrt.dot(L).dot(Dinvsqrt)  # Normalized graph laplacian\n",
    "                \n",
    "#                 assert(is_pos_def(Dinvsqrt))\n",
    "#                 assert(is_pos_def(L))\n",
    "                \n",
    "                yield L.shape[0], L\n",
    "            \n",
    "            num_questions += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load question answer similarity values from file (probably should compute it here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_similarity_features(filepath):\n",
    "    features = []\n",
    "    labels = []\n",
    "    map_label = {\"positive\": 1, \"negative\": 0}\n",
    "    with open(filepath) as infile:\n",
    "        for line in infile:\n",
    "            label, score = line.strip().split(',')\n",
    "            score = float(score)\n",
    "            label = map_label[label]\n",
    "            features.append(score)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return np.asarray(features).reshape(-1, 1), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import training data and test data\n",
    "train_datapath = \"../myclassify/qa.train.arff\"\n",
    "test_datapath = \"../myclassify/qa.test.arff\"\n",
    "\n",
    "X_train, y_train = importData(train_datapath)\n",
    "X_test, y_test = importData(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached EOF at qn 100\n"
     ]
    }
   ],
   "source": [
    "train_file = \"../data/answerSelectionExperiments/data/train-less-than-40.xml\"\n",
    "test_file = \"../data/answerSelectionExperiments/data/test-less-than-40.xml\"\n",
    "qn_group_indicators = get_QA_group_indicators(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Propagate rank score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cvxpy import Variable, Minimize, norm, quad_form, Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate_scores(r, L, alpha=1.0):\n",
    "    \"\"\"Solve convex optimization problem to get new scores\"\"\"\n",
    "        \n",
    "    n = r.size\n",
    "    y = Variable(n)\n",
    "    \n",
    "    # TODO: Add huber loss\n",
    "    objective = Minimize( norm(r - y, 1) + alpha * quad_form(y, L) )\n",
    "#     objective = Minimize( norm(r - y, 1) + alpha * quad_form(y, L) )\n",
    "    \n",
    "    constraints = [0 <= y, y <= 1]\n",
    "    prob = Problem(objective, constraints)\n",
    "\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve()        \n",
    "    return y.value.flatten().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_propagation(data_filepath, r, alpha=1.0, sigma=1.0, n_neighbors=5):\n",
    "    total_count = 0\n",
    "    one_count = 0\n",
    "\n",
    "    # Get qn group indicator\n",
    "    qn_group_indicators = get_QA_group_indicators(data_filepath)\n",
    "    qn_number = 0  # Current question number (NOTE: This is not ID in XML)\n",
    "\n",
    "    scores = []\n",
    "    for (count, L) in get_weight_matrix(data_filepath, n_neighbors, sigma):\n",
    "        # Skip question without candidate answers\n",
    "        if count is None:\n",
    "            continue\n",
    "\n",
    "        # Not enough points to propagate. Just use original value.\n",
    "        if count == 1:\n",
    "            one_count += 1                \n",
    "            assert(r[qn_group_indicators == qn_number].size == 1)\n",
    "            # Append original score\n",
    "            scores.append(r[qn_group_indicators == qn_number][0])        \n",
    "        else:\n",
    "            # Propagate and append new scores\n",
    "            assert(r[qn_group_indicators == qn_number].size == L.shape[0])\n",
    "            new_scores = propagate_scores(r[qn_group_indicators == qn_number] , L, alpha)                \n",
    "            scores += new_scores\n",
    "\n",
    "        qn_number += 1\n",
    "        total_count += count\n",
    "\n",
    "    print(\"one_count: {} total_count: {}\".format(one_count, total_count))\n",
    "\n",
    "    return np.asarray(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=300, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=300, max_depth=12, criterion=\"gini\")\n",
    "clf_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached EOF at qn 95\n",
      "one_count: 14 total_count: 1517\n",
      "0.828609096902\n",
      "0.837837837838\n",
      "0.00996321110855\n",
      "0.472836722479\n",
      "260\n",
      "0.171390903098\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "\n",
    "# clf = linear_model.LogisticRegression(C=0.005)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "raw_scores = clf.predict_proba(X_test)[:, 1]\n",
    "# raw_scores = clf_rf.predict_proba(X_test)[:, 1]\n",
    "# scores = rank_propagation(test_file, raw_scores, alpha=2, sigma=1, n_neighbors=9)\n",
    "scores = rank_propagation(test_file, raw_scores, alpha=3.0, sigma=1.0, n_neighbors=11)\n",
    "\n",
    "y_pred_adjusted = (scores >= 0.5)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, scores >= 0.5))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(np.sum(np.abs(raw_scores - scores)) / len(scores))  # Average difference between actual\n",
    "print(np.max(np.abs(raw_scores - scores)))\n",
    "print(np.sum(y_test != (scores >= 0.5)))\n",
    "print(np.sum(y_test != (scores >= 0.5)) / float(len(scores)))\n",
    "\n",
    "print(np.sum(np.abs(y_pred - y_pred_adjusted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = np.hstack(((1 - scores).reshape(-1, 1), scores.reshape(-1, 1)))\n",
    "predict_for_test(y_test, y_pred_adjusted, P, \"LR_adjusted.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_for_test(y_test, y_pred, clf.predict_proba(X_test), \"LR.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached EOF at qn 93\n",
      "one_count: 5 total_count: 4718\n",
      "0.93069097075\n",
      "0.00652913182896\n"
     ]
    }
   ],
   "source": [
    "# For training\n",
    "train_raw_scores = clf.predict_proba(X_train)[:, 1]\n",
    "scores = rank_propagation(train_file, train_raw_scores, alpha=2, sigma=2, n_neighbors=11)\n",
    "\n",
    "print(accuracy_score(y_train, scores >= 0.5))\n",
    "print(np.sum(np.abs(train_raw_scores - scores)) / len(scores))  # Average difference between actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93535396354387457"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_adjusted = (scores >= 0.5)\n",
    "np.sum(y_pred - y_pred_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16545814106789716"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "251.0 / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83783783783783783"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
