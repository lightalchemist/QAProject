{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Compute Type features for TREC dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for installing Stanford NER tagger etc. for NLTK\n",
    "https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#stanford-tagger-ner-tokenizer-and-parser\n",
    "\n",
    "Stanford Core NLP tools can be tried online through their web-based interface: http://corenlp.run\n",
    "\n",
    "NER extraction using Python NLTK: http://nlpforhackers.io/named-entity-extraction/\n",
    "\n",
    "Penn Tree bank POS tags list: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "Information on chunking: http://web.media.mit.edu/~havasi/MAS.S60/PNLP7.pdf\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls ../data/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NERTagger\n",
    "stanford_dir = '/Users/hongwei/nltk_data/models/stanford-ner-2016-10-31/'\n",
    "jarfile = stanford_dir + 'stanford-ner.jar'\n",
    "# modelfile = stanford_dir + 'classifiers/english.conll.4class.distsim.crf.ser.gz'\n",
    "modelfile = stanford_dir + 'classifiers/english.muc.7class.distsim.crf.ser.gz'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=modelfile, path_to_jar=jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StanfordNERTagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'It', u'O'),\n",
       " (u'was', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'first', u'O'),\n",
       " (u'Fortune', u'O'),\n",
       " (u'<', u'O'),\n",
       " (u'num', u'O'),\n",
       " (u'>', u'O'),\n",
       " (u'company', u'O'),\n",
       " (u'to', u'O'),\n",
       " (u'endorse', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'principles', u'O'),\n",
       " (u'of', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'Coalition', u'ORGANIZATION'),\n",
       " (u'of', u'ORGANIZATION'),\n",
       " (u'Environmentally', u'ORGANIZATION'),\n",
       " (u'Responsible', u'ORGANIZATION'),\n",
       " (u'Economies', u'ORGANIZATION'),\n",
       " (u',', u'O'),\n",
       " (u'a', u'O'),\n",
       " (u'non-profit', u'O'),\n",
       " (u'organisation', u'O'),\n",
       " (u'set', u'O'),\n",
       " (u'up', u'O'),\n",
       " (u'after', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'Exxon', u'O'),\n",
       " (u'Valdez', u'O'),\n",
       " (u'oil-spill', u'O'),\n",
       " (u'to', u'O'),\n",
       " (u'promote', u'O'),\n",
       " (u'green', u'O'),\n",
       " (u'business', u'O'),\n",
       " (u'policies', u'O'),\n",
       " (u'.', u'O')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"It\twas\tthe\tfirst\tFortune\t<num>\tcompany\tto\tendorse\tthe\tprinciples\tof\tthe\tCoalition\tof\tEnvironmentally\tResponsible\tEconomies\t,\ta\tnon-profit\torganisation\tset\tup\tafter\tthe\tExxon\tValdez\toil-spill\tto\tpromote\tgreen\tbusiness\tpolicies\t.\"\n",
    "# sentence = \"Let's meet on wednesday at 9am.\"\n",
    "# sentence = \"Sun\thas\tgone a long way in setting itself apart from its\trivals, too.\"\n",
    "# sentence = \"The\ttwo\tfunds\thave\ttotal\tassets\tof\tabout\t$\t<num>\tbillion.\"\n",
    "# sentence = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "# sentence = \"Let's meet tomorrow at 9pm\";\n",
    "\n",
    "sentence = sentence.replace('\\t', ' ')\n",
    "# print(sentence)\n",
    "# sentence = \"Margaret Tatcher was the first woman Prime Minister of England\"\n",
    "st.tag(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'While', u'O'),\n",
       " (u'in', u'O'),\n",
       " (u'France', u'LOCATION'),\n",
       " (u',', u'O'),\n",
       " (u'Christine', u'PERSON'),\n",
       " (u'Lagarde', u'PERSON'),\n",
       " (u'discussed', u'O'),\n",
       " (u'short-term', u'O'),\n",
       " (u'stimulus', u'O'),\n",
       " (u'efforts', u'O'),\n",
       " (u'in', u'O'),\n",
       " (u'a', u'O'),\n",
       " (u'recent', u'O'),\n",
       " (u'interview', u'O'),\n",
       " (u'with', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'Wall', u'O'),\n",
       " (u'Street', u'O'),\n",
       " (u'Journal', u'O'),\n",
       " (u'.', u'O'),\n",
       " (u'Lets', u'O'),\n",
       " (u'meet', u'O'),\n",
       " (u'on', u'O'),\n",
       " (u'wednesday', u'DATE'),\n",
       " (u'at', u'O'),\n",
       " (u'9pm', u'O'),\n",
       " (u'.', u'O')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal. Lets meet on wednesday at 9pm.'\n",
    "st.tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  While/IN\n",
      "  in/IN\n",
      "  (GPE France/NNP)\n",
      "  ,/,\n",
      "  (PERSON Christine/NNP Lagarde/NNP)\n",
      "  discussed/VBD\n",
      "  short-term/JJ\n",
      "  stimulus/NN\n",
      "  efforts/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  recent/JJ\n",
      "  interview/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  (FACILITY Wall/NNP Street/NNP Journal/NNP)\n",
      "  ./.\n",
      "  (PERSON Lets/NNP)\n",
      "  meet/NN\n",
      "  on/IN\n",
      "  wednesday/NN\n",
      "  at/IN\n",
      "  9pm/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print ne_chunk(pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'IN', u'O'), ('in', 'IN', u'O'), ('France', 'NNP', u'B-GPE'), (',', ',', u'O'), ('Christine', 'NNP', u'B-PERSON'), ('Lagarde', 'NNP', u'I-PERSON'), ('discussed', 'VBD', u'O'), ('short-term', 'JJ', u'O'), ('stimulus', 'NN', u'O'), ('efforts', 'NNS', u'O'), ('in', 'IN', u'O'), ('a', 'DT', u'O'), ('recent', 'JJ', u'O'), ('interview', 'NN', u'O'), ('with', 'IN', u'O'), ('the', 'DT', u'O'), ('Wall', 'NNP', u'B-FACILITY'), ('Street', 'NNP', u'I-FACILITY'), ('Journal', 'NNP', u'I-FACILITY'), ('.', '.', u'O'), ('Lets', 'NNP', u'B-PERSON'), ('meet', 'NN', u'O'), ('on', 'IN', u'O'), ('wednesday', 'NN', u'O'), ('at', 'IN', u'O'), ('9pm', 'CD', u'O'), ('.', '.', u'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "# sentence = \"Let's meet tomorrow at 9 pm\";\n",
    "# text = \"What are the Valdez Principles?\"\n",
    "# sentence = \"Mark and John are working at Google.\"\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    " \n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "print iob_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in ('\\f', '\\n', '\\r', '\\t', '\\v'): #strip whitespaces\n",
    "    text = text.replace(s, '')\n",
    "text += '\\n' #ensure end-of-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal. Lets meet on wednesday at 9pm.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'What', u'O'),\n",
       " (u'are', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'Valdez', u'O'),\n",
       " (u'Principles', u'O'),\n",
       " (u'?', u'O')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.tag(word_tokenize(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
