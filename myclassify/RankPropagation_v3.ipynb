{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output in Weka format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def predict_for_test(test, predict, probability, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"=== Predictions on test data ===\\n\")\n",
    "        f.write(\" inst#     actual  predicted error prediction\\n\")\n",
    "        for i in range(len(test)):\n",
    "            string = [str(i + 1)]\n",
    "            if test[i] == 1:\n",
    "                string.append(\"1:positive\")\n",
    "            else:\n",
    "                string.append(\"2:negative\")\n",
    "            if predict[i] == 1:\n",
    "                string.append(\"1:positive\")\n",
    "            else:\n",
    "                string.append(\"2:negative\")\n",
    "            if test[i] == predict[i]:\n",
    "                string.append(\" \" * 5)\n",
    "            else:\n",
    "                string.append(\" \" * 2 + \"+\" + \" \" * 2)\n",
    "            if predict[i] == 1:\n",
    "                string.append(str(probability[i][1]))\n",
    "            else:\n",
    "                string.append(str(probability[i][0]))\n",
    "            string = \" \".join(string) + \"\\n\"\n",
    "            f.write(string)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "#import data\n",
    "def importData(path):\n",
    "    dataset = arff.load(open(path, 'rb'))\n",
    "    data = np.array(dataset['data'])\n",
    "    #print data[:10]\n",
    "\n",
    "    #extract features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        f = []\n",
    "        for i in range(len(d) - 1):\n",
    "            num = float(d[i])\n",
    "            if int(num) == num:\n",
    "                num = int(num)\n",
    "            f.append(num)\n",
    "        features.append(f)\n",
    "\n",
    "        if d[-1] == \"positive\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return np.asarray(features), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map each word to an index\n",
    "ndim = 300\n",
    "glove_path = \"../data/glove_embeddings/glove.6B.{}d.txt\".format(ndim)\n",
    "with open(glove_path, \"rb\") as lines:\n",
    "    w2idx = {line.split()[0].decode(\"utf-8\"): i for i, line in enumerate(lines)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = np.empty((len(w2idx), ndim), dtype=np.float)\n",
    "with open(glove_path, \"rb\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "        vectors[i] = np.asarray(map(float, line.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words_to_exclude = frozenset(string.punctuation) | frozenset([\"..\", \"...\"])\n",
    "words_to_exclude |= frozenset(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rank propagation\n",
    "\n",
    "---\n",
    "Get vector indicating which example belongs to which question group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_QA_group_count(infile):        \n",
    "    line = infile.readline().strip()\n",
    "    if line == \"\":\n",
    "        return None\n",
    "    \n",
    "    if not line.startswith(\"<QApairs\"):\n",
    "        raise Exception(\"Invalid data format: {}<-----\".format(line))\n",
    "    \n",
    "    sentence_count = 0\n",
    "    while not line.strip().startswith(\"</QApairs\"):\n",
    "        line = infile.readline().replace('\\t', ' ')                    \n",
    "        if line.strip().lower().startswith(\"<positive\") or line.strip().lower().startswith(\"<negative\"):\n",
    "            sentence_count += 1\n",
    "    \n",
    "    return sentence_count\n",
    "\n",
    "def get_QA_group_indicators(filepath):    \n",
    "    with open(filepath) as infile:\n",
    "        indicators = []\n",
    "        qn_number = 0\n",
    "        while infile:\n",
    "            count = get_QA_group_count(infile)\n",
    "\n",
    "            # Check for EOF\n",
    "            if count == None:\n",
    "                break\n",
    "\n",
    "            if count > 0:\n",
    "                indicators += ([qn_number] * count)\n",
    "                qn_number += 1\n",
    "        \n",
    "    return np.asarray(indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a question and its candidate answers\n",
    "def get_QA_group(infile):\n",
    "    question = []\n",
    "    answers = []\n",
    "    line = infile.readline().strip()\n",
    "    if line == \"\":\n",
    "        return None\n",
    "    \n",
    "    if not line.startswith(\"<QApairs\"):\n",
    "        raise Exception(\"Invalid data format: {}<-----\".format(line))\n",
    "        \n",
    "    while not line.strip().startswith(\"</QApairs\"):\n",
    "        line = infile.readline().replace('\\t', ' ')\n",
    "        if line.strip().lower().startswith(\"<question\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            question.append(line.strip())\n",
    "        elif line.strip().lower().startswith(\"<positive\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            answers.append((\"positive\", line.strip()))\n",
    "        elif line.strip().lower().startswith(\"<negative\"):\n",
    "            line = infile.readline().replace('\\t', ' ')\n",
    "            answers.append((\"negative\", line.strip()))\n",
    "    \n",
    "    return {\"question\": question, \"answers\": answers}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def extract_vector(sentence, exclude, w2idx, wordvectors):\n",
    "    \"\"\"Compute the vector for a sentence by averaging the words in the sentence that has word embeddings\"\"\"\n",
    "    # Tokenize sentence\n",
    "    splitter = WhitespaceTokenizer()\n",
    "    tokens = splitter.tokenize(sentence)    \n",
    "    # Remove stopwords and punctuation\n",
    "    words = [t.lower() for t in tokens if t.lower() not in exclude ]\n",
    "    \n",
    "    # If we cannot find any words, we can consider returning a vector of 0\n",
    "    # and set the resulting cosine similarity to 0 otherwise will result in nan\n",
    "    # because cosine similarity will divide by 0.\n",
    "    assert(len(words) > 0)\n",
    "            \n",
    "    # Average words in sentence that are in word matrix\n",
    "    try:\n",
    "        avg_vec = np.mean([wordvectors[w2idx[w]] for w in words if w in w2idx ] \n",
    "                                                 or [np.zeros(wordvectors.shape[1])], \n",
    "                           axis=0)\n",
    "        if not np.any(avg_vec):\n",
    "            print(\"Tokens cannot be found: {}\".format(words))\n",
    "        assert(np.any(avg_vec))\n",
    "        return avg_vec\n",
    "    except UnicodeDecodeError:\n",
    "        print(line.strip())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pairwise_distance_matrix(X, k, p=2):\n",
    "    \"\"\"Compute pairwise distances between each point in X\n",
    "    and its k-nearest neighbors.\"\"\"\n",
    "\n",
    "    from scipy.spatial import KDTree\n",
    "    kdtree = KDTree(X)\n",
    "    A = np.zeros((X.shape[0], X.shape[0]), dtype=np.float)\n",
    "    for i, x in enumerate(X):\n",
    "        distances, idxs = kdtree.query(x, k+1, p)  # k+1 as one pt is the pt itself.\n",
    "        for d, j in zip(distances, idxs):\n",
    "            A[i, j] = d\n",
    "\n",
    "    # p = 2 corresponds to gaussian kernel. p = 1 corresponds to Laplacian kernel.\n",
    "    if p == 2:  # Store squared euclidean for L2 distance otherwise if p = 1 just store absolute dist.\n",
    "        A = A ** 2\n",
    "\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute weight matrix (i.e., the Graph Laplacian) $L$ for each set of question and its candidate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# Compute the scores/features for a dataset\n",
    "def get_weight_matrix(input_file, n_neighbors=5, sigma=1.0, eps=0.0001, p=2):\n",
    "    \"\"\"Compute weight matrix for question and answer sentences \n",
    "    \"\"\"\n",
    "    with open(input_file) as infile:\n",
    "        num_questions = 0\n",
    "        while infile:\n",
    "            group = get_QA_group(infile)\n",
    "\n",
    "            # Check for EOF\n",
    "            if group is None:\n",
    "                break\n",
    "                \n",
    "            # Extract question vector\n",
    "            question = group[\"question\"]\n",
    "            qvec = extract_vector(question[0], words_to_exclude, w2idx, vectors)\n",
    "\n",
    "            scores = []\n",
    "            answer_vectors = []\n",
    "            for (label, sentence) in group[\"answers\"]:\n",
    "                # Compute similarity with question vector\n",
    "                vec = extract_vector(sentence, words_to_exclude, w2idx, vectors) # TODO: Pass these in as args\n",
    "                answer_vectors.append(vec)\n",
    "                cosine_distance = sp.spatial.distance.cosine(qvec, vec)\n",
    "                scores.append((label, cosine_distance))\n",
    "\n",
    "            # Compute pairwise distances between the answer vectors for K nearest neighbor\n",
    "            k = min(n_neighbors, len(answer_vectors) - 1) # Minus 1 because have to exclude itself\n",
    "            # Not enough to do rank propagation. Just keep original scores.\n",
    "            if k < 0:\n",
    "                yield None, None\n",
    "            elif k == 0:\n",
    "                yield 1, None\n",
    "            else:                \n",
    "                answer_vectors = np.vstack(answer_vectors)\n",
    "                W = compute_pairwise_distance_matrix(answer_vectors, k, p)\n",
    "                W = np.maximum(W, W.T)  # Ensure W symmetric.\n",
    "                W[W > 0] = np.exp(- W[W > 0] / (2 * sigma**2))  # Apply gaussian kernel\n",
    "                D = np.diag(np.sum(W, axis=1))  # Row sum of W\n",
    "                L = D - W\n",
    "#                 L = L + eps * np.eye(len(answer_vectors))  # Improve the condition of the graph laplacian                \n",
    "                Dinvsqrt = np.sqrt(np.linalg.pinv(D))                \n",
    "                # Need to ensure that Dinvsqrt does not have NAN due to division by 0\n",
    "                assert(not np.any(np.isnan(Dinvsqrt)))                \n",
    "                L = Dinvsqrt.dot(L).dot(Dinvsqrt)  # Normalized graph laplacian\n",
    "                \n",
    "#                 assert(is_pos_def(Dinvsqrt))\n",
    "#                 assert(is_pos_def(L))\n",
    "                \n",
    "                yield L.shape[0], L\n",
    "            \n",
    "            num_questions += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load question answer similarity values from file (probably should compute it here).\n",
    "\n",
    "**NOTE**: Similarity is the wrong term to use here. The values in the file are actually cosine **distances**.\n",
    "To convert it to similarity, we need to subtract it from 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_similarity_features(filepath):\n",
    "    features = []\n",
    "    labels = []\n",
    "    map_label = {\"positive\": 1, \"negative\": 0}\n",
    "    with open(filepath) as infile:\n",
    "        for line in infile:\n",
    "            label, score = line.strip().split(',')\n",
    "            score = float(score)\n",
    "            label = map_label[label]\n",
    "            features.append(score)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return np.asarray(features).reshape(-1, 1), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import training data and test data\n",
    "train_datapath = \"../myclassify/qa.train.arff\"\n",
    "dev_datapath = \"../myclassify/qa.dev.arff\"\n",
    "test_datapath = \"../myclassify/qa.test.arff\"\n",
    "\n",
    "X_train, y_train = importData(train_datapath)\n",
    "X_dev, y_dev = importData(dev_datapath)\n",
    "X_test, y_test = importData(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = \"../data/answerSelectionExperiments/data/train-less-than-40.xml\"\n",
    "dev_file = \"../data/answerSelectionExperiments/data/dev-less-than-40.xml\"\n",
    "test_file = \"../data/answerSelectionExperiments/data/test-less-than-40.xml\"\n",
    "qn_group_indicators = get_QA_group_indicators(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sim_train, y_sim_train = load_similarity_features(\"../data/features/glove_embedding_sentence_similarities_train_100.txt\")\n",
    "X_sim_test, y_sim_test = load_similarity_features(\"../data/features/glove_embedding_sentence_similarities_test_100.txt\")\n",
    "\n",
    "X_combined_train = np.hstack((X_train, X_sim_train))\n",
    "X_combined_test = np.hstack((X_test, X_sim_test))\n",
    "\n",
    "# Scale combined data\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_combined_train)\n",
    "X_comb_scaled_train = scaler.transform(X_combined_train)\n",
    "X_comb_scaled_test = scaler.transform(X_combined_test)\n",
    "\n",
    "# Only normalize the similarity scores\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_sim_train)\n",
    "X_sim_train = scaler.transform(X_sim_train)\n",
    "X_sim_test = scaler.transform(X_sim_test)\n",
    "X_comb_scaledsim_train = np.hstack((X_train, X_sim_train))\n",
    "X_comb_scaledsim_test = np.hstack((X_test, X_sim_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Propagate rank score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cvxpy import Variable, Minimize, norm, quad_form, Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    \"\"\"Check if a matrix is positive definite. For debugging purposes.\"\"\"\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Version 2 that uses the ans_type_match and question answer similarity weights\n",
    "def propagate_scores(r, L, ans_type_match, ans_sim_weights, alpha=1.0, gamma=1.0, loss_type=1):\n",
    "    \"\"\"Solve convex optimization problem to get new scores\"\"\"\n",
    "        \n",
    "    n = r.size\n",
    "    y = Variable(n)    \n",
    "    assert(len(ans_type_match) == n and len(ans_sim_weights) == n)\n",
    "    \n",
    "    # If no type match we just ignore the type term\n",
    "    if not np.any(ans_type_match):\n",
    "        objective = Minimize( norm(r - y, loss_type) + alpha * quad_form(y, L) )\n",
    "    else:\n",
    "        type_term = sum( ans_sim_weights[i] * cvxpy.abs(1 - y[i]) \n",
    "                        for i, match in enumerate(ans_type_match) if match == 1 )\n",
    "        objective = Minimize( norm(r - y, loss_type) + alpha * quad_form(y, L) + gamma * type_term)\n",
    "            \n",
    "    constraints = [0 <= y, y <= 1]\n",
    "    prob = Problem(objective, constraints)    \n",
    "\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve(verbose=False)      \n",
    "    assert(prob.status == \"optimal\")\n",
    "    \n",
    "    return y.value.flatten().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qn_answer_match_indicators(filepath):\n",
    "    with open(filepath) as infile:\n",
    "        return np.asarray([int(x.strip()) for x in infile])\n",
    "    \n",
    "def get_qn_answer_sim_weights(filepath):\n",
    "    with open(filepath) as infile:\n",
    "        return np.asarray([float(x.split(',')[1]) for x in infile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_propagation(data_filepath, qn_match_filepath, qn_simweights_filepath, r, \n",
    "                     alpha=1.0, sigma=1.0, n_neighbors=5, gamma=1.0, \n",
    "                     loss_type=1, \n",
    "                     pair_similarity_type=2):\n",
    "    total_count = 0\n",
    "\n",
    "    # Get qn group indicator\n",
    "    qn_group_indicators = get_QA_group_indicators(data_filepath)\n",
    "    \n",
    "    # Get qn answer type match\n",
    "    qn_ans_type_match = get_qn_answer_match_indicators(qn_match_filepath)\n",
    "    \n",
    "    # Get qn answer similarity weights\n",
    "    # The weights are actually distances so we subtract them from 1 to convert distance to similarity\n",
    "    qn_ans_sim_weights = 1 - get_qn_answer_sim_weights(qn_simweights_filepath)\n",
    "    \n",
    "    qn_number = 0  # Current question number (NOTE: This is not ID in XML)\n",
    "\n",
    "    scores = []  # To store the final refined scores\n",
    "    # L is actually the graph Laplacian matrix\n",
    "    for (count, L) in get_weight_matrix(data_filepath, \n",
    "                                        n_neighbors, sigma, \n",
    "                                        p=pair_similarity_type):\n",
    "        # Skip question without candidate answers\n",
    "        if count is None:\n",
    "            continue\n",
    "\n",
    "        # Not enough points to propagate. Just use original value.\n",
    "        MIN_NUM_CANDIDATES = 1\n",
    "        if count <= MIN_NUM_CANDIDATES:\n",
    "            assert(r[qn_group_indicators == qn_number].size == count)\n",
    "            scores += r[qn_group_indicators == qn_number].tolist()\n",
    "        else:\n",
    "            # Get indicator vector for which answer has matching type\n",
    "            ans_type_match = qn_ans_type_match[qn_group_indicators == qn_number]\n",
    "            \n",
    "            # Get question / answer similarity weights\n",
    "            ans_sim_weights = qn_ans_sim_weights[qn_group_indicators == qn_number]                        \n",
    "            \n",
    "            # Propagate and append new scores\n",
    "            assert(r[qn_group_indicators == qn_number].size == L.shape[0])\n",
    "            new_scores = propagate_scores(r[qn_group_indicators == qn_number],\n",
    "                                          L, \n",
    "                                          ans_type_match, ans_sim_weights,\n",
    "                                          alpha, gamma, loss_type)                \n",
    "            scores += new_scores\n",
    "\n",
    "        qn_number += 1\n",
    "        total_count += count\n",
    "\n",
    "    return np.asarray(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic classifier to give input ranks for Rank Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with using RandomTreeEmbedding before passing to LR. Also possible to try kernel embedding for explicit nonlinear feature mapping before applying LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomtreesembedding', RandomTreesEmbedding(max_depth=3, max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=400, n_jobs=1, random_state=3713,\n",
       "           sparse_output=True, verbo...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rt = RandomTreesEmbedding(max_depth=3, n_estimators=400,\n",
    "                          random_state=3713)\n",
    "\n",
    "rt_lm = LogisticRegression(C=0.01)\n",
    "pipeline = make_pipeline(rt, rt_lm)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82267633487145686"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pipeline.predict_proba(X_test)[:, 1]\n",
    "P = np.hstack(((1 - scores).reshape(-1, 1), scores.reshape(-1, 1)))\n",
    "predict_for_test(y_test, y_pred, P, \"rt_embed_LR.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code that uses Random Forest to perform embedding before training LR does not work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.73566249176\n",
      "f1: 0.305025996534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "# nb = GaussianNB()\n",
    "nb = BernoulliNB(alpha=2)\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"f1: {}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.825972313777\n",
      "f1: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Probably don't try this.\n",
    "# GBM\n",
    "gbm = GradientBoostingClassifier(n_estimators=400, max_depth=12, random_state=47156)\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"f1: {}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.826631509558\n",
      "f1: 0.344139650873\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "rf = RandomForestClassifier(n_estimators=400, \n",
    "                                criterion=\"entropy\", \n",
    "                                max_depth=11,\n",
    "                                class_weight=\"balanced_subsample\",\n",
    "                                random_state=73514)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"f1: {}\".format(f1_score(y_test, y_pred)))\n",
    "# predict_for_test(y_test, y_pred, rf.predict_proba(X_test), \"rf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.827290705339\n",
      "f1: 0.238372093023\n"
     ]
    }
   ],
   "source": [
    "# LR\n",
    "# lr = LogisticRegression(C=3, class_weight=\"balanced\")\n",
    "# lr = LogisticRegression(C=0.01)\n",
    "lr = LogisticRegression(C=100, max_iter=1e8)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"f1: {}\".format(f1_score(y_test, y_pred)))\n",
    "predict_for_test(y_test, y_pred, rf.predict_proba(X_test), \"lr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sim_train, y_sim_train = load_similarity_features(\"../data/features/glove_embedding_sentence_similarities_train_100.txt\")\n",
    "X_sim_test, y_sim_test = load_similarity_features(\"../data/features/glove_embedding_sentence_similarities_test_100.txt\")\n",
    "\n",
    "# Only normalize the similarity scores\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_sim_train)\n",
    "X_sim_train = scaler.transform(X_sim_train)\n",
    "X_sim_test = scaler.transform(X_sim_test)\n",
    "X_comb_scaledsim_train = np.hstack((X_train, X_sim_train))\n",
    "X_comb_scaledsim_test = np.hstack((X_test, X_sim_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.836519446276\n",
      "f1: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10, max_iter=1e8)\n",
    "lr.fit(X_comb_scaledsim_train, y_train)\n",
    "y_pred = lr.predict(X_comb_scaledsim_test)\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"f1: {}\".format(f1_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rank Propagation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted accuracy: 0.734344100198\n",
      "Original accuracy: 0.73566249176\n",
      "Adjusted f1: 0.342577487765\n",
      "Original f1: 0.305025996534\n",
      "0.0315161780508\n",
      "0.941206191389\n",
      "403\n",
      "0.265655899802\n",
      "Number of disagreement: 36\n",
      "Adjusted\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8493    0.8183    0.8335      1233\n",
      "          1     0.3191    0.3697    0.3426       284\n",
      "\n",
      "avg / total     0.7501    0.7343    0.7416      1517\n",
      "\n",
      "Original\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8399    0.8337    0.8368      1233\n",
      "          1     0.3003    0.3099    0.3050       284\n",
      "\n",
      "avg / total     0.7389    0.7357    0.7372      1517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "\n",
    "# Set this to one of the trained classifiers above (after training)\n",
    "clf = nb\n",
    "\n",
    "# This file contains entries indicating whether answer has a entity matching type required by answer prediction\n",
    "# Line corresponding to entry has 1 if there is a match, else 0\n",
    "qn_match_filepath = \"../data/QuestionType/test_answer_type_match.txt\"\n",
    "\n",
    "# This file contains the cosine *distance* between question and answer\n",
    "qn_simweights_filepath = \"../data/features/glove_embedding_sentence_similarities_test_300.txt\"\n",
    "\n",
    "raw_scores = clf.predict_proba(X_test)[:, 1]\n",
    "raw_full_scores = clf.predict_proba(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# These are used with best model\n",
    "# raw_scores = clf.predict_proba(X_comb_scaledsim_test)[:, 1]\n",
    "# raw_full_scores = clf.predict_proba(X_comb_scaledsim_test)\n",
    "# y_pred = clf.predict(X_comb_scaledsim_test)\n",
    "\n",
    "# I think we have to keep the number of neighbors in the graph small because there are only a few\n",
    "# positive examples. We don't want to link them to too many negative ones.\n",
    "# This works well with 300 dim glove vector\n",
    "# scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "#                           raw_scores, alpha=1, sigma=1.0, n_neighbors=3, gamma=1.5)\n",
    "\n",
    "# L2 loss for | r - y | gives 0.6377 and 0.755 \n",
    "# scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "#                           raw_scores, alpha=0.5, sigma=1, n_neighbors=3, gamma=1.5,\n",
    "#                           loss_type=1,\n",
    "#                           pair_similarity_type=2)\n",
    "\n",
    "# MRR = 0.7474 vs 0.7399, MAP = 0.6462, 0.6272 for pair_similarity_type=2, sigma=1.0\n",
    "# Generated with LR trained with C = 100, max_iter = 10000\n",
    "# map            all 0.6448\n",
    "# recip_rank     all 0.7537\n",
    "# scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "#                           raw_scores, alpha=0.5, sigma=2.0, n_neighbors=3, gamma=1.5,\n",
    "#                           loss_type=1, \n",
    "#                           pair_similarity_type=1)\n",
    "\n",
    "# This with a LR trained with C = 2 gives MRR = 0.7674 MAP = 0.6518\n",
    "# For this to work, we have to use X_comb_scaledsim_test\n",
    "# scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "#                           raw_scores, alpha=2.0, sigma=1.0, n_neighbors=5, gamma=1.0,\n",
    "#                           loss_type=1, \n",
    "#                           pair_similarity_type=2)\n",
    "\n",
    "scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "                          raw_scores, alpha=1.0, sigma=1.0, n_neighbors=3, gamma=1.5,\n",
    "                          loss_type=1, \n",
    "                          pair_similarity_type=2)\n",
    "\n",
    "y_pred_adjusted = (scores >= 0.5)\n",
    "\n",
    "print(\"Adjusted accuracy: {}\".format(accuracy_score(y_test, scores >= 0.5)))\n",
    "print(\"Original accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Adjusted f1: {}\".format(f1_score(y_test, scores >= 0.5)))\n",
    "print(\"Original f1: {}\".format(f1_score(y_test, y_pred)))\n",
    "print(np.sum(np.abs(raw_scores - scores)) / len(scores))  # Average difference between actual\n",
    "print(np.max(np.abs(raw_scores - scores)))\n",
    "print(np.sum(y_test != (scores >= 0.5)))\n",
    "print(np.sum(y_test != (scores >= 0.5)) / float(len(scores)))\n",
    "print(\"Number of disagreement: {}\".format(np.sum(np.abs(y_pred - y_pred_adjusted))))\n",
    "print(\"Adjusted\")\n",
    "print(classification_report(y_test, y_pred_adjusted, digits=4))\n",
    "print(\"Original\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converts to weka format\n",
    "P = np.hstack(((1 - scores).reshape(-1, 1), scores.reshape(-1, 1)))\n",
    "predict_for_test(y_test, y_pred_adjusted, P, \"nb_type_term_adjusted.txt\")\n",
    "predict_for_test(y_test, y_pred, raw_full_scores, \"nb_no_adjust.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR with C = 1, max_iter = 10000\n",
    "scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "                          raw_scores, alpha=0.5, sigma=1, n_neighbors=3, gamma=1.5,\n",
    "                          loss_type=1,\n",
    "                          pair_similarity_type=2)\n",
    "\n",
    "Adjusted\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.85      0.98      0.91      1233\n",
    "          1       0.74      0.26      0.38       284\n",
    "\n",
    "avg / total       0.83      0.84      0.81      1517\n",
    "\n",
    "Original\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.83      0.98      0.90      1233\n",
    "          1       0.69      0.15      0.24       284\n",
    "\n",
    "avg / total       0.81      0.83      0.78      1517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRR = 0.7474 vs 0.7399, MAP = 0.6462, 0.6272\n",
    "LR trained with C = 100, max_iter = 10000\n",
    "\n",
    "scores = rank_propagation(test_file, qn_match_filepath, qn_simweights_filepath, \n",
    "                          raw_scores, alpha=0.5, sigma=1, n_neighbors=3, gamma=1.5,\n",
    "                          loss_type=1, \n",
    "                          pair_similarity_type=2)\n",
    "                          \n",
    "Adjusted\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.85      0.98      0.91      1233\n",
    "          1       0.73      0.24      0.36       284\n",
    "\n",
    "avg / total       0.83      0.84      0.81      1517\n",
    "\n",
    "Original\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.83      0.98      0.90      1233\n",
    "          1       0.68      0.14      0.24       284\n",
    "\n",
    "avg / total       0.81      0.83      0.78      1517\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached EOF at qn 93\n",
      "one_count: 5 total_count: 4718\n",
      "0.93069097075\n",
      "0.00652913182896\n"
     ]
    }
   ],
   "source": [
    "# For training\n",
    "train_raw_scores = clf.predict_proba(X_train)[:, 1]\n",
    "scores = rank_propagation(train_file, train_raw_scores, alpha=2, sigma=2, n_neighbors=11)\n",
    "\n",
    "print(accuracy_score(y_train, scores >= 0.5))\n",
    "print(np.sum(np.abs(train_raw_scores - scores)) / len(scores))  # Average difference between actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93535396354387457"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, clf.predict(X_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
